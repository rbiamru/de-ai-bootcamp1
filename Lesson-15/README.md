# Lesson 15: Enhancing OpenAI Assistants with RAG

In our previous lesson, we explored OpenAI Assistants and their ability to create preconfigured chatbots for thread and message management. However, we recognized that these AI models have significant limitations in "reasoning" and context size, potentially leading to suboptimal performance on complex tasks involving large amounts of information.

Today, we'll delve into Retrieval-Augmented Generation (RAG) and how it can expand the capabilities of these models. RAG enables the retrieval of information from external sources such as files, web pages, videos, books, and other documents without encountering the hard context size limit of these models.

By the end of this lesson, we'll create an application using the Vercel AI SDK to leverage assistants via API calls to the OpenAI Platform for thread creation and message management.

## Prerequisites

- Proficiency in using a shell/terminal/console/bash on your device
  - Familiarity with basic commands like `cd`, `ls`, and `mkdir`
  - Ability to execute packages, scripts, and commands
  - [Node.js](https://nodejs.org/en/download/)
- Proficiency with `npm` and `npx` commands
  - Documentation: [npm](https://docs.npmjs.com/)
  - Documentation: [npx](https://www.npmjs.com/package/npx)
- Understanding of `npm install` and managing the `node_modules` folder
  - Documentation: [npm install](https://docs.npmjs.com/cli/v10/commands/npm-install)
- Git CLI installed on your device
  - [Git](https://git-scm.com/downloads)
- Proficiency with `git` commands for cloning repositories
  - Documentation: [Git](https://git-scm.com/doc)
- Basic knowledge of JavaScript programming language syntax
  - [JavaScript official tutorial](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide)
  - [Learn X in Y minutes](https://learnxinyminutes.com/docs/javascript/)
- Basic knowledge of TypeScript programming language syntax
  - [TypeScript official tutorial](https://www.typescriptlang.org/docs/)
  - [Learn X in Y minutes](https://learnxinyminutes.com/docs/typescript/)
- Account at [OpenAI Platform](https://platform.openai.com/)
  - For running API Commands on the platform, set up [billing](https://platform.openai.com/account/billing/overview) and add at least **5 USD** credits to your account

## Review of Lesson 14

- Low-Rank Adaptations (LoRAs)
- Comparative analysis of text generation models
- Code Generation models
- Addressing GPT limitations
- OpenAI Assistants

## Retrieving Information

## Retrieval-Augmented Generation (RAG)

- **Definition and Purpose**
  - RAG is an advanced technique that enhances GPT capabilities by integrating external information retrieval
  - It enables GPTs to access and utilize data from diverse sources such as files, web pages, videos, books, and other documents
  - This approach allows the model to generate responses based on external content, even when it's not part of the model's original training data
  - RAG bridges the gap between static model knowledge and dynamic, up-to-date information

- **Context Size Management**
  - Challenge: GPTs have a limited context size, restricting the amount of information that can be processed in a single prompt
  - Solution: Instead of including entire external sources in the prompt context, RAG employs intelligent fragment selection
  - Process:
    1. The system analyzes the user's query to "understand" the information need
    2. It then selects the most relevant portions of augmenting data for that specific query
    3. These selected fragments are incorporated into the prompt context
  - Benefit: This method enables the GPT to leverage extensive external sources without exceeding its context size limitations
  - Efficiency: By focusing on relevant information, RAG improves response quality and reduces computational overhead

- **Information Retrieval from Large Datasets**
  - Data Preparation:
    1. Divide provided data into manageable _chunks_ (typically paragraphs or small sections)
    2. Store these chunks in an organized database structure (often a vector database)
    3. Index all chunks and calculate their embeddings to create informative _metadata_
    4. Embeddings are high-dimensional vector representations that capture semantic meaning
  - Retrieval Process:
    1. When a prompt is received, the system analyzes it to create a query embedding
    2. A vector search algorithm calculates similarity between the query embedding and chunk embeddings
    3. The most relevant chunks are selected based on this similarity score
    4. Selected chunks are then incorporated into the prompt context
  - Advanced Techniques:
    - Hybrid retrieval combining keyword and semantic search for improved accuracy
    - Re-ranking of retrieved chunks to further refine relevance

- **Integration of Retrieved Information**
  - When relevant chunks are appended to the context, the model's fine-tuning for instructions enables it to adapt its behavior
  - The model generates responses that incorporate the newly provided information, effectively expanding its knowledge base for each query
  - This process allows for dynamic, context-aware responses that blend the model's inherent knowledge with retrieved information

- **Key Benefits**
  - Expanded Knowledge: Allows GPTs to access information beyond their training data
  - Dynamic Responses: Enables more accurate and up-to-date answers by incorporating external sources
  - Efficient Processing: Manages large datasets without overwhelming the model's context capacity
  - Improved Accuracy: Focuses on the most relevant information for each specific query
  - Reduced Hallucination: By grounding responses in retrieved facts, RAG can minimize the model's tendency to generate incorrect information

- **Challenges and Considerations**
  - Quality of Retrieved Information: The system's effectiveness depends on the accuracy and relevance of the retrieved data
  - Balancing Retrieval and Generation: Finding the right mix of retrieved information and model-generated content
  - Computational Overhead: RAG systems require additional processing for retrieval, which can impact response times
  - Ethical Considerations: Ensuring the retrieved information is from reliable and unbiased sources

## Practical Application of RAG with OpenAI Assistants

- Exercise 1: [Create an Assistant with File-Based Information Retrieval](./exercises/00-RAG-Assistant.md)
  - Objective: Create an assistant, provide a file for information retrieval, and test it with a message thread.

## Developing a RAG-Enhanced Chat Application with OpenAI Assistants

- OpenAI Assistants features
  - [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter)
  - [File Search](https://platform.openai.com/docs/assistants/tools/file-search)
    - [Vector Stores](https://platform.openai.com/docs/assistants/tools/file-search/vector-stores)
  - [Function Calling](https://platform.openai.com/docs/assistants/tools/function-calling)
    - [Function Call Workflow](https://platform.openai.com/docs/guides/function-calling)
    - Using [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction) for [Function Calling](https://platform.openai.com/docs/guides/function-calling/function-calling-with-structured-outputs)
- Utilizing [Vercel AI SDK UI](https://sdk.vercel.ai/docs/ai-sdk-ui/openai-assistants) assistants hook
- Implementing RAG to enhance application capabilities
  - Reference: [Example RAG chatbot](https://sdk.vercel.ai/docs/guides/rag-chatbot)
- Application development
  - Using OpenAI [Vercel SDK Quickstart Template](https://vercel.com/templates/next.js/openai-assistants-quickstart) for Assistants
- Deployment via Vercel

- Practical exercise
  - Exercise 2: [Create a chat application with OpenAI Assistants](./exercises/01-RAG-Chat-Application.md)

## Next Steps

- Advanced RAG implementation techniques
- File chunking strategies
- Understanding and utilizing embeddings
- Vector database search techniques for relevant information retrieval
- Creating a RAG-based Documentation Chatbot using custom data
- Introduction to Model Context Protocol (MCP)
- Weekend project implementation

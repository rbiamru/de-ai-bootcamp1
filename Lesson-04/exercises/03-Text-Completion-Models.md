# OpenAI's API using text completion models

> Before starting, make sure that you have set up your keys correctly with the exercise `01-OpenAI-Key.md`
>
> > Check if you have `python` (or `py`) and `pip` installed on your machine

1. Setup a `Virtual Environment` for the project

   - [Tutorial](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)
   - Run `python -m venv venv/` (Linux/MacOS) or `py -m venv venv/` (Windows) to create a virtual environment on your project's folder

2. Activate the virtual environment

   - venv [docs](https://docs.python.org/3/library/venv.html)
   - Run `source venv/bin/activate` or `. venv/bin/activate` (Linux/MacOS) or `venv/Scripts/activate` (Windows) to activate the virtual environment

3. Run the following code on your terminal:

   ```bash
   pip install openai
   ```

   - This command will install the `openai` [API Package](https://github.com/openai/openai-python) on your environment

4. Create a new python file

   - Create a new file on your favorite code editor or simply run `touch <filename>.py` on your terminal (Linux/MacOS) or `type nul > <filename>.py` on your terminal (Windows)

   - Remember to replace `<filename>` with the name of your file

   - Open the file on your preferred code editor

5. Import the `openai` module on your file

   ```python
    from openai import OpenAI
   ```

   - This `client` can abstract all of the complexities of consuming the OpenAI API endpoints, like handling the authentication, the request and response formats, synchronous and asynchronous requests, and many other features

   - To use this library correctly, all you need to do is to understand well the [API parameters](https://platform.openai.com/docs) that you want to consume

6. Create a new `client` instance

   ```python
    client = OpenAI()
   ```

   - By default, this will try to use the `OPENAI_API_KEY` environment variable to create this client

   - You can customize the logic by doing an explicit definition like this:

   ```python
   import os
   from openai import OpenAI
   client = OpenAI(
       api_key=os.environ.get("OPENAI_API_KEY"),
   )
   ```

   - Here you can change `api_key` to any value that you want to use

   - Have caution if you prefer to hardcode your key in this file, since this could lead to you inadvertently sharing your key with others

7. Write a variable with the `prompt` to test in the loop:

   ```python
   messages = [
       {
           "role": "user",
           "content": "Say this is a test",
       }
   ]
   ```

8. Print the `prompt` message in the console:

   ```python
   print(f"Prompt:\n{messages[0]['content']}\n")
   ```

9. Define an array with the GPT's models:

   ```python
   models = ["gpt-3.5-turbo", "gpt-4", "gpt-4-0125-preview"]
   ```

10. Call the [Chat Completion](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) endpoint in the `client` in a loop with the following parameters:

    ```python
    for model in models:
        print(f"\n---\nGenerating chat completion with {model}:\n")
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True,
        )
        for chunk in stream:
            print(chunk.choices[0].delta.content or "", end="")
    ```

    - The for loop will iterate each element of models that you've put in the array, and will and configure a stream to that model

    - The `model` parameter is the name of the model that you want to use to generate the text

      - The models available to use from OpenAI API are listed in the [Models](https://platform.openai.com/docs/models/overview) API documentation

    - The `messages` parameter is a list of messages that you want to use to use as `prompt` generate the text

      - This example is using a set of instructions for separating the _roles_ of different _actors_ in the chat conversation

    - The `stream` parameter is a boolean that indicates if the response should be streamed or not

      - By default the API will wait for the model inference to be completed and return the result in a single response

    - If you set this parameter to `True`, the API will return a `stream` object that you can use to read the response chunks by chunks, as they are generated by the server running the model

    - This function will print the response chunks as they are generated by the server running the model:

      ```python
      for chunk in stream:
          print(chunk.choices[0].delta.content or "", end="")
      ```

11. Use the following prompt for your chat completions:

    ```text
    Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.

    Mention the large language model based product mentioned in the paragraph above:
    ```

    - Or use this prompt:

      ```text
      Sarah has 5 brothers. Each of Sarah's brothers has 2 sisters. How many sisters does Sarah have in total?
      ```

12. Run the file

    ```bash
    # Linux/MacOS
    python <filename>.py
    ```

    ```bash
    # Windows
    py <filename>.py
    ```

13. Compare the results from each model

    - Test other prompts as well to see the difference in their responses

    - You'll note that each model has a different response to the same prompt

14. Notice in the [Models Documentation](https://platform.openai.com/docs/models/overview) that each model has a different training data cut-off

    - `gpt-3.5-turbo`: Up to Sep 2021

    - `gpt-4`: Up to Sep 2021

    - `gpt-4-0125-preview`: Up to Dec 2023

15. Experiment with prompts mentioning historical events, considering the training data cut-off of each model

    - As an example try to use as prompt:

    ```text
    When did the current war between Russia and Ukraine started?
    ```

16. Notice that each model has different performance characteristics and related costs of execution

    - `gpt-3.5-turbo`: This is one of the most cost-efficient model that can perform relatively well on a variety of tasks

      - Fine-tuning can improve its performance, making it match or even outperform base GPT-4-level capabilities on certain narrow tasks

    - `gpt-4`: GPT-4 is one of the first models to exhibit human-level performance on various professional and academic benchmarks

      - For example, it passes a simulated bar exam with a score around the top 10% of test-takers2

      - It is more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5

    - `gpt-4o`: A most recent _reasoning_ model fine-tuned to perform operations of emulated reasoning before answering prompts

17. Notice that each model has a different [pricing](https://openai.com/pricing) of execution per token, usually related to the model's performance and capabilities, and consequently to the associated costs of developing and running it

    - Use this example prompt to test the cost of execution per token for each model:

    ```text
    Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.

    Mention the large language model based product mentioned in the paragraph above:
    ```

    - Notice that text has a input of 112 tokens using the OpenAI GPT encoding algorithm

      - You can check the number of tokens in a text passed to ChatGPT using the [Tokenizer](https://platform.openai.com/tokenizer) tool

    - Costs for `gpt-3.5-turbo`:

      - Input: The cost for input is $0.50 per 1M tokens, thus totaling **$0.000056** for 112 tokens

      - Output: The output is very short and consisted of only 3 tokens to state `ChatGPT`

        - Considering the cost of $1.50 per 1M tokens, the output cost is **$0.00000045**

      - Total: **$0.00005645**

    - `gpt-4`:

      - Input: The cost for input using $30 per 1M tokens, thus totaling **$0.00336** for 112 tokens

      - Output: The output is longer and consisted of 16 tokens to state `The large language model based product mentioned in the paragraph above is ChatGPT.`

        - considering the cost of $60 per 1M tokens, the output cost is **$0.000256**

      - Total: **$0.003616**

    - `gpt-4-0125-preview`:

      - Input: The cost for input using $10 per 1M tokens, thus totaling **$0.00112** for 112 tokens

      - Output: The output is the longest and consisted of 20 tokens to state `The large language model (LLM) based product mentioned in the paragraph above is ChatGPT.`

        - considering the cost of $30 per 1M tokens, the output cost is **$0.0006**

      - Total: **$0.00172**

    - Considering that your AI App can have thousands of operations like this from many different users, the costs can add up quickly, and cost efficiency can become a predominant factor in the choice of which model to use

> For more information about each model, refer to <https://platform.openai.com/docs/models>
